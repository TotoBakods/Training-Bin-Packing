{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step-by-Step Warehouse Optimization Model Training\n",
                "\n",
                "This notebook guides you through the process of training Machine Learning models for 3D warehouse packing. \n",
                "We will cover:\n",
                "1.  **Setup**: Installing dependencies and cloning the repo (or assuming files are uploaded).\n",
                "2.  **Data Generation**: Creating synthetic packing scenarios (Heuristic Labeling).\n",
                "3.  **Model Training**: Training Neural Networks to mimic these strategies.\n",
                "4.  **Evaluation**: Validating the models with metrics.\n",
                "5.  **Download**: Getting the trained models.\n",
                "\n",
                "**Note**: If running in Colab, ensure you upload the project files or clone your repository."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install torch numpy pandas flask flask-cors scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "\n",
                "# Check CUDA availability\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Create directories\n",
                "os.makedirs('training_data', exist_ok=True)\n",
                "os.makedirs('models', exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.5 Prepare Dataset & Train GAN\n",
                "\n",
                "First, we convert the raw `bed-bpp` JSON dataset into a CSV format suitable for training the Generative Adversarial Network (GAN). Then, we train the GAN to learn the distributions of real-world items."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!python tools/convert_dataset.py\n",
                "!python gan/train.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generating Training Data (Labeling)\n",
                "\n",
                "We need to create a dataset of \"good\" packing solutions. We use existing heuristic algorithms (Genetic Algorithm, Extremal Optimization) to solve thousands of random packing problems. The resulting $(x, y, z)$ coordinates become the labels our neural network will learn to predict.\n",
                "\n",
                "*Note: This step can take a while (10-30 mins) depending on sample count.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import your project modules\n",
                "# Ideally, you should have uploaded generate_training_data.py, optimizer.py, etc.\n",
                "# If not, this cell assumes you are in the project root.\n",
                "\n",
                "import random\n",
                "import csv\n",
                "import uuid\n",
                "from optimizer import GeneticAlgorithm, ExtremalOptimization, HybridOptimizer\n",
                "\n",
                "# Configuration for Colab speed\n",
                "OUTPUT_DIR = \"training_data\"\n",
                "SAMPLES_PER_ALGO = 50  # Generate 50 scenarios per algo\n",
                "ITEMS_PER_SAMPLE = 20  \n",
                "\n",
                "\n",
                "import sys\n",
                "if './gan' not in sys.path:\n",
                "    sys.path.append('./gan')\n",
                "import pickle\n",
                "import random\n",
                "try:\n",
                "    from model import Generator\n",
                "except ImportError:\n",
                "    from gan.model import Generator\n",
                "\n",
                "def generate_gan_items(count, warehouse_dims):\n",
                "    l_wh, w_wh, h_wh = warehouse_dims\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    \n",
                "    scaler_path = 'gan/scaler.pkl'\n",
                "    ckpt_path = 'gan/checkpoints/generator.pth'\n",
                "    \n",
                "    with open(scaler_path, 'rb') as f:\n",
                "        scaler = pickle.load(f)\n",
                "        \n",
                "    model = Generator(100, 4).to(device)\n",
                "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
                "    model.eval()\n",
                "    \n",
                "    z = torch.randn(count, 100).to(device)\n",
                "    with torch.no_grad():\n",
                "        gen_data = model(z).cpu().numpy()\n",
                "        \n",
                "    original_data = scaler.inverse_transform(gen_data)\n",
                "    \n",
                "    items = []\n",
                "    categories = ['General', 'Electronics', 'Clothing']\n",
                "    fragile_categories = {'Electronics'}\n",
                "    \n",
                "    for i in range(count):\n",
                "        l, w, h, weight = original_data[i]\n",
                "        l, w, h, weight = abs(l)*2.0, abs(w)*2.0, abs(h)*2.0, abs(weight)*2.0\n",
                "        \n",
                "        cat = random.choice(categories)\n",
                "        is_fragile = 1 if cat in fragile_categories else 0\n",
                "        is_stackable = 0 if is_fragile else (1 if random.random() > 0.1 else 0)\n",
                "        can_rotate = 0 if h > 2 * min(l, w) else 1\n",
                "        \n",
                "        items.append({\n",
                "            'id': str(uuid.uuid4()),\n",
                "            'length': round(float(l), 2), 'width': round(float(w), 2), 'height': round(float(h), 2),\n",
                "            'weight': round(float(weight), 2), 'category': cat,\n",
                "            'can_rotate': can_rotate, 'stackable': is_stackable, 'fragility': is_fragile, 'access_freq': random.randint(1,10),\n",
                "            'x': 0, 'y': 0, 'z': 0, 'rotation': 0\n",
                "        })\n",
                "    return items\n",
                "\n",
                "def run_data_generation():\n",
                "    print(\"Starting Data Generation...\")\n",
                "    algorithms = [\n",
                "        ('fit_eo', ExtremalOptimization(iterations=20)),\n",
                "        ('fit_ga', GeneticAlgorithm(population_size=20, generations=10)),\n",
                "        ('fit_eo_ga', HybridOptimizer(ga_generations=1, eo_iterations=2)),\n",
                "        ('fit_ga_eo', HybridOptimizer(ga_generations=5, eo_iterations=10))\n",
                "    ]\n",
                "    \n",
                "    weights = {'space': 0.6, 'accessibility': 0.1, 'stability': 0.3}\n",
                "\n",
                "    for algo_name, optimizer in algorithms:\n",
                "        print(f\"Generating for {algo_name}...\")\n",
                "        csv_path = os.path.join(OUTPUT_DIR, f\"{algo_name}.csv\")\n",
                "        \n",
                "        with open(csv_path, 'w', newline='') as f:\n",
                "            writer = csv.writer(f)\n",
                "            header = ['item_l', 'item_w', 'item_h', 'weight', 'fragile', 'stackable', 'can_rotate',\n",
                "                      'wh_l', 'wh_w', 'wh_h', 'target_x', 'target_y', 'target_z', 'target_rot']\n",
                "            writer.writerow(header)\n",
                "            \n",
                "            for _ in range(SAMPLES_PER_ALGO):\n",
                "                # Randomize Warehouse Size\n",
                "                wh_l = round(random.uniform(10, 30), 1)\n",
                "                wh_w = round(random.uniform(10, 30), 1)\n",
                "                wh_h = round(random.uniform(5, 15), 1)\n",
                "                \n",
                "                warehouse = {'id': 1, 'length': wh_l, 'width': wh_w, 'height': wh_h}\n",
                "                items = generate_gan_items(ITEMS_PER_SAMPLE, (wh_l, wh_w, wh_h))\n",
                "                \n",
                "                try:\n",
                "                    if 'Hybrid' in str(type(optimizer)) and algo_name == 'fit_eo_ga':\n",
                "                        sol, _, _ = optimizer.optimize_eo_ga(items, warehouse, weights)\n",
                "                    else:\n",
                "                        sol, _, _ = optimizer.optimize(items, warehouse, weights)\n",
                "                    \n",
                "                    # Write valid placements\n",
                "                    for idx, sol_item in enumerate(sol):\n",
                "                        item = items[idx]\n",
                "                        writer.writerow([\n",
                "                            item['length'], item['width'], item['height'], item['weight'], \n",
                "                            item['fragility'], item['stackable'], item['can_rotate'],\n",
                "                            wh_l, wh_w, wh_h,\n",
                "                            sol_item['x'], sol_item['y'], sol_item['z'], sol_item['rotation']\n",
                "                        ])\n",
                "                except Exception as e:\n",
                "                    print(f\"Skipping sample error: {e}\")\n",
                "                    \n",
                "    print(\"Data Generation Complete.\")\n",
                "\n",
                "run_data_generation()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train Models\n",
                "\n",
                "Now we train 4 separate Neural Networks (one for each strategy) using the CSV data we just generated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import pandas as pd\n",
                "import glob\n",
                "\n",
                "# Define Model Architecture (Same as project ml_utils.py)\n",
                "class PackingModel(nn.Module):\n",
                "    def __init__(self, input_dim=10, output_dim=4):\n",
                "        super(PackingModel, self).__init__()\n",
                "        self.fc1 = nn.Linear(input_dim, 64)\n",
                "        self.fc2 = nn.Linear(64, 128)\n",
                "        self.fc3 = nn.Linear(128, 64)\n",
                "        self.fc4 = nn.Linear(64, output_dim)\n",
                "        self.relu = nn.ReLU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.relu(self.fc2(x))\n",
                "        x = self.relu(self.fc3(x))\n",
                "        out = self.fc4(x)\n",
                "        return out\n",
                "\n",
                "# Dataset Wrapper\n",
                "class WarehouseDataset(Dataset):\n",
                "    def __init__(self, csv_file):\n",
                "        self.data = pd.read_csv(csv_file)\n",
                "        # Inputs: item dims (3) + weight + flags (3) + wh dims (3) = 10\n",
                "        self.x = self.data.iloc[:, 0:10].values.astype('float32')\n",
                "        # Targets: x, y, z, rot\n",
                "        self.y = self.data.iloc[:, 10:14].values.astype('float32')\n",
                "        \n",
                "        # Normalize Inputs (Approximate)\n",
                "        self.x[:, 0:3] /= 10.0  # Item dims\n",
                "        self.x[:, 7:10] /= 50.0 # Warehouse dims\n",
                "        \n",
                "        # Normalize Targets (Relative to Warehouse)\n",
                "        wh_l = self.data['wh_l'].values.astype('float32')\n",
                "        wh_w = self.data['wh_w'].values.astype('float32')\n",
                "        wh_h = self.data['wh_h'].values.astype('float32')\n",
                "        \n",
                "        self.y[:, 0] /= (wh_l + 1e-5)\n",
                "        self.y[:, 1] /= (wh_w + 1e-5)\n",
                "        self.y[:, 2] /= (wh_h + 1e-5)\n",
                "        self.y[:, 3] /= 6.0 # Rotation normalized\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
                "\n",
                "def train_all_models():\n",
                "    csv_files = glob.glob(os.path.join(OUTPUT_DIR, \"*.csv\"))\n",
                "    \n",
                "    for csv_file in csv_files:\n",
                "        model_name = os.path.basename(csv_file).replace('.csv', '')\n",
                "        print(f\"\\nTraining model: {model_name}...\")\n",
                "        \n",
                "        dataset = WarehouseDataset(csv_file)\n",
                "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
                "        \n",
                "        model = PackingModel().to(device)\n",
                "        criterion = nn.MSELoss()\n",
                "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "        \n",
                "        for epoch in range(20): # 20 Epochs for demo\n",
                "            total_loss = 0\n",
                "            for bx, by in dataloader:\n",
                "                bx, by = bx.to(device), by.to(device)\n",
                "                optimizer.zero_grad()\n",
                "                pred = model(bx)\n",
                "                loss = criterion(pred, by)\n",
                "                loss.backward()\n",
                "                optimizer.step()\n",
                "                total_loss += loss.item()\n",
                "            \n",
                "            if (epoch+1) % 5 == 0:\n",
                "                print(f\"Epoch {epoch+1}/20 - Loss: {total_loss/len(dataloader):.6f}\")\n",
                "        \n",
                "        # Save\n",
                "        save_path = os.path.join(\"models\", f\"model_{model_name}.pth\")\n",
                "        torch.save(model.state_dict(), save_path)\n",
                "        print(f\"Saved to {save_path}\")\n",
                "\n",
                "train_all_models()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation and Metrics\n",
                "\n",
                "We evaluate the trained models by running inference on test packing scenarios and calculating key performance indicators: Space Utilization, Accessibility, and Stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import os\n",
                "\n",
                "# Simple Physics/Metrics Logic (Simplified from optimizer.py)\n",
                "def calculate_metrics(solution, items, wh_dims):\n",
                "    # solution: list of dicts {x, y, z, rotation}\n",
                "    # items: list of dicts\n",
                "    \n",
                "    # 1. Space Utilization\n",
                "    total_item_vol = sum(i['length'] * i['width'] * i['height'] for i in items)\n",
                "    wh_vol = wh_dims[0] * wh_dims[1] * wh_dims[2]\n",
                "    space_util = total_item_vol / wh_vol\n",
                "    \n",
                "    # 2. Stability (Simplified: check if z > 0 has support)\n",
                "    stable_count = 0\n",
                "    for i, sol in enumerate(solution):\n",
                "        if sol['z'] <= 0.01: # On floor\n",
                "            stable_count += 1\n",
                "        else:\n",
                "            # Check if supported by another item\n",
                "            supported = False\n",
                "            for j, other in enumerate(solution):\n",
                "                if i == j: continue\n",
                "                if abs(other['z'] + items[j]['height'] - sol['z']) < 0.05: # Vertical contact\n",
                "                    # Check horizontal overlap\n",
                "                    if (abs(sol['x'] - other['x']) < (items[i]['length'] + items[j]['length'])/2 and\n",
                "                        abs(sol['y'] - other['y']) < (items[i]['width'] + items[j]['width'])/2):\n",
                "                        supported = True\n",
                "                        break\n",
                "            if supported:\n",
                "                stable_count += 1\n",
                "    stability = stable_count / len(solution)\n",
                "    \n",
                "    # 3. Accessibility (Distance to door at 0,0)\n",
                "    dists = [np.sqrt(s['x']**2 + s['y']**2) for s in solution]\n",
                "    accessibility = 1.0 / (1.0 + np.mean(dists))\n",
                "    \n",
                "    return space_util, accessibility, stability\n",
                "\n",
                "def evaluate_model(model_name):\n",
                "    print(f\"\\nEvaluating {model_name}...\")\n",
                "    model_path = os.path.join(\"models\", f\"{model_name}.pth\")\n",
                "    if not os.path.exists(model_path):\n",
                "        print(\"Model not found.\")\n",
                "        return\n",
                "        \n",
                "    model = PackingModel().to(device)\n",
                "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
                "    model.eval()\n",
                "    \n",
                "    # Generate Test Case\n",
                "    wh_l, wh_w, wh_h = 20.0, 20.0, 10.0\n",
                "    items = generate_gan_items(20, (wh_l, wh_w, wh_h))\n",
                "    \n",
                "    # Prepare Input\n",
                "    features = []\n",
                "    for item in items:\n",
                "        features.append([\n",
                "            item['length']/10.0, item['width']/10.0, item['height']/10.0,\n",
                "            item['weight']/50.0, item['fragility'], item['stackable'], item['can_rotate'],\n",
                "            wh_l/50.0, wh_w/50.0, wh_h/50.0\n",
                "        ])\n",
                "    \n",
                "    inputs = torch.tensor(features, dtype=torch.float32).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(inputs).cpu().numpy()\n",
                "        \n",
                "    # Decode Output\n",
                "    solution = []\n",
                "    for i, row in enumerate(outputs):\n",
                "        solution.append({\n",
                "            'x': row[0] * wh_l,\n",
                "            'y': row[1] * wh_w,\n",
                "            'z': max(0, row[2] * wh_h), # Floor clamp\n",
                "            'rotation': int(row[3] * 6)\n",
                "        })\n",
                "        \n",
                "    # Calculate Metrics\n",
                "    sp, acc, stb = calculate_metrics(solution, items, (wh_l, wh_w, wh_h))\n",
                "    print(f\"  Space Utilization: {sp*100:.2f}%\")\n",
                "    print(f\"  Stability Score:   {stb*100:.2f}%\")\n",
                "    print(f\"  Accessibility:     {acc:.4f}\")\n",
                "\n",
                "# Run Eval\n",
                "evaluate_model('model_fit_ga')\n",
                "evaluate_model('model_fit_eo')\n",
                "evaluate_model('model_fit_eo_ga')\n",
                "evaluate_model('model_fit_ga_eo')\n",
                ""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Download Models\n",
                "Zip the trained models so you can download them and use them in your local Flask app."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!zip -r trained_models.zip models\n",
                "\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download('trained_models.zip')\n",
                "except ImportError:\n",
                "    print(\"Not running in Colab environment. Files are in 'models/' directory.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}